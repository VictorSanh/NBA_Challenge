{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, BatchNormalization, Dropout, Flatten, Dense, LSTM, Bidirectional\n",
    "from keras import losses\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from src.utils import preprocess, feature_engineering\n",
    "random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function src.utils.feature_engineering>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open files\n",
    "Data_X_train = pd.read_csv('data/challenge_fichier_dentrees_dentrainement_challenge_nba/train.csv')\n",
    "Data_Y_train = pd.read_csv('data/challenge_fichier_de_sortie_dentrainement_challenge_nba.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1439/1439 [00:19<00:00, 74.56it/s]\n",
      "100%|██████████| 1440/1440 [01:22<00:00, 10.38it/s]\n",
      "100%|██████████| 1440/1440 [04:24<00:00,  5.44it/s] \n"
     ]
    }
   ],
   "source": [
    "Data_X_train = feature_engineering(Data_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Data_X_train.as_matrix()[:, 1:]\n",
    "Y_train = Data_Y_train.as_matrix()[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Data_X_train, Data_Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12576, 23040), 16.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, 23040/1440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn = X_train.reshape((len(X_train), 16, 1440, 1), order = 'F')\n",
    "Y_train_cnn = np_utils.to_categorical(Y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_3 (Batch (None, 16, 1440, 1)       64        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 287, 16)        2576      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 287, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4592)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                229650    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 232,392\n",
      "Trainable params: 232,360\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_model = Sequential()\n",
    "conv_model.add(BatchNormalization(axis=1, \n",
    "                                  input_shape = (16, 1440, 1)))\n",
    "conv_model.add(Conv2D(filters = 16, \n",
    "                 kernel_size = (16, 10),\n",
    "                 strides = (1, 5),\n",
    "                 activation = 'relu'))\n",
    "conv_model.add(Dropout(0.75))\n",
    "conv_model.add(Flatten())\n",
    "conv_model.add(Dense(units = 50, activation = 'relu'))\n",
    "conv_model.add(Dropout(0.5))\n",
    "conv_model.add(Dense(units = 2, \n",
    "                activation='softmax'))\n",
    "conv_model.compile(loss = losses.categorical_crossentropy,\n",
    "                     optimizer = 'adam',\n",
    "                     metrics = ['accuracy'])\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10689 samples, validate on 1887 samples\n",
      "Epoch 1/200\n",
      "10689/10689 [==============================] - 12s 1ms/step - loss: 0.6048 - acc: 0.6825 - val_loss: 0.5774 - val_acc: 0.6969\n",
      "Epoch 2/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5752 - acc: 0.7049 - val_loss: 0.5612 - val_acc: 0.6963\n",
      "Epoch 3/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5703 - acc: 0.7028 - val_loss: 0.5575 - val_acc: 0.7054\n",
      "Epoch 4/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5723 - acc: 0.7080 - val_loss: 0.5679 - val_acc: 0.7160\n",
      "Epoch 5/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5693 - acc: 0.7079 - val_loss: 0.5590 - val_acc: 0.7181\n",
      "Epoch 6/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5632 - acc: 0.7122 - val_loss: 0.5522 - val_acc: 0.7175\n",
      "Epoch 7/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5654 - acc: 0.7142 - val_loss: 0.5534 - val_acc: 0.7154\n",
      "Epoch 8/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5678 - acc: 0.7112 - val_loss: 0.5595 - val_acc: 0.7128\n",
      "Epoch 9/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5651 - acc: 0.7106 - val_loss: 0.5530 - val_acc: 0.7154\n",
      "Epoch 10/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5650 - acc: 0.7107 - val_loss: 0.5595 - val_acc: 0.7059\n",
      "Epoch 11/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5655 - acc: 0.7095 - val_loss: 0.5499 - val_acc: 0.7154\n",
      "Epoch 12/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5598 - acc: 0.7078 - val_loss: 0.5572 - val_acc: 0.7038\n",
      "Epoch 13/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5631 - acc: 0.7125 - val_loss: 0.5565 - val_acc: 0.7027\n",
      "Epoch 14/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5580 - acc: 0.7109 - val_loss: 0.5549 - val_acc: 0.7101\n",
      "Epoch 15/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5560 - acc: 0.7122 - val_loss: 0.5541 - val_acc: 0.7075\n",
      "Epoch 16/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5574 - acc: 0.7146 - val_loss: 0.5577 - val_acc: 0.7128\n",
      "Epoch 17/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5619 - acc: 0.7156 - val_loss: 0.5515 - val_acc: 0.7213\n",
      "Epoch 18/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5566 - acc: 0.7122 - val_loss: 0.5522 - val_acc: 0.7154\n",
      "Epoch 19/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5560 - acc: 0.7157 - val_loss: 0.5586 - val_acc: 0.7138\n",
      "Epoch 20/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5566 - acc: 0.7146 - val_loss: 0.5527 - val_acc: 0.7122\n",
      "Epoch 21/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5540 - acc: 0.7160 - val_loss: 0.5529 - val_acc: 0.7096\n",
      "Epoch 22/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5533 - acc: 0.7200 - val_loss: 0.5518 - val_acc: 0.7038\n",
      "Epoch 23/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5630 - acc: 0.7086 - val_loss: 0.5543 - val_acc: 0.7101\n",
      "Epoch 24/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5567 - acc: 0.7186 - val_loss: 0.5558 - val_acc: 0.7175\n",
      "Epoch 25/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5557 - acc: 0.7166 - val_loss: 0.5502 - val_acc: 0.7154\n",
      "Epoch 26/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5554 - acc: 0.7224 - val_loss: 0.5515 - val_acc: 0.7128\n",
      "Epoch 27/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5537 - acc: 0.7164 - val_loss: 0.5531 - val_acc: 0.7091\n",
      "Epoch 28/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5503 - acc: 0.7194 - val_loss: 0.5507 - val_acc: 0.7170\n",
      "Epoch 29/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5548 - acc: 0.7139 - val_loss: 0.5510 - val_acc: 0.7160\n",
      "Epoch 30/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5547 - acc: 0.7181 - val_loss: 0.5545 - val_acc: 0.7186\n",
      "Epoch 31/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5497 - acc: 0.7185 - val_loss: 0.5499 - val_acc: 0.7170\n",
      "Epoch 32/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5500 - acc: 0.7169 - val_loss: 0.5642 - val_acc: 0.7059\n",
      "Epoch 33/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5507 - acc: 0.7221 - val_loss: 0.5576 - val_acc: 0.7128\n",
      "Epoch 34/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5540 - acc: 0.7231 - val_loss: 0.5548 - val_acc: 0.7080\n",
      "Epoch 35/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5506 - acc: 0.7232 - val_loss: 0.5523 - val_acc: 0.7144\n",
      "Epoch 36/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5529 - acc: 0.7153 - val_loss: 0.5558 - val_acc: 0.6995\n",
      "Epoch 37/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5474 - acc: 0.7203 - val_loss: 0.5537 - val_acc: 0.7085\n",
      "Epoch 38/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5511 - acc: 0.7179 - val_loss: 0.5490 - val_acc: 0.7165\n",
      "Epoch 39/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5494 - acc: 0.7209 - val_loss: 0.5567 - val_acc: 0.7181\n",
      "Epoch 40/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5494 - acc: 0.7194 - val_loss: 0.5523 - val_acc: 0.7117\n",
      "Epoch 41/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5466 - acc: 0.7213 - val_loss: 0.5504 - val_acc: 0.7096\n",
      "Epoch 42/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5435 - acc: 0.7280 - val_loss: 0.5517 - val_acc: 0.7133\n",
      "Epoch 43/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5458 - acc: 0.7230 - val_loss: 0.5522 - val_acc: 0.7186\n",
      "Epoch 44/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5442 - acc: 0.7177 - val_loss: 0.5516 - val_acc: 0.7154\n",
      "Epoch 45/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5434 - acc: 0.7263 - val_loss: 0.5523 - val_acc: 0.7170\n",
      "Epoch 46/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5457 - acc: 0.7235 - val_loss: 0.5507 - val_acc: 0.7154\n",
      "Epoch 47/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5460 - acc: 0.7248 - val_loss: 0.5627 - val_acc: 0.7032\n",
      "Epoch 48/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5548 - acc: 0.7152 - val_loss: 0.5547 - val_acc: 0.7122\n",
      "Epoch 49/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5455 - acc: 0.7264 - val_loss: 0.5581 - val_acc: 0.7133\n",
      "Epoch 50/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5462 - acc: 0.7221 - val_loss: 0.5579 - val_acc: 0.7107\n",
      "Epoch 51/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5447 - acc: 0.7214 - val_loss: 0.5545 - val_acc: 0.7133\n",
      "Epoch 52/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5459 - acc: 0.7213 - val_loss: 0.5560 - val_acc: 0.7133\n",
      "Epoch 53/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5465 - acc: 0.7230 - val_loss: 0.5524 - val_acc: 0.7213\n",
      "Epoch 54/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5416 - acc: 0.7260 - val_loss: 0.5610 - val_acc: 0.7038\n",
      "Epoch 55/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5440 - acc: 0.7259 - val_loss: 0.5590 - val_acc: 0.6990\n",
      "Epoch 56/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5436 - acc: 0.7267 - val_loss: 0.5631 - val_acc: 0.7122\n",
      "Epoch 57/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5427 - acc: 0.7253 - val_loss: 0.5529 - val_acc: 0.7091\n",
      "Epoch 58/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5438 - acc: 0.7261 - val_loss: 0.5532 - val_acc: 0.7117\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5396 - acc: 0.7262 - val_loss: 0.5495 - val_acc: 0.7186\n",
      "Epoch 60/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5410 - acc: 0.7265 - val_loss: 0.5524 - val_acc: 0.7101\n",
      "Epoch 61/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5443 - acc: 0.7221 - val_loss: 0.5511 - val_acc: 0.7154\n",
      "Epoch 62/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5379 - acc: 0.7278 - val_loss: 0.5551 - val_acc: 0.7154\n",
      "Epoch 63/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5347 - acc: 0.7278 - val_loss: 0.5555 - val_acc: 0.7154\n",
      "Epoch 64/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5436 - acc: 0.7246 - val_loss: 0.5558 - val_acc: 0.7064\n",
      "Epoch 65/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5424 - acc: 0.7226 - val_loss: 0.5579 - val_acc: 0.7107\n",
      "Epoch 66/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5367 - acc: 0.7301 - val_loss: 0.5548 - val_acc: 0.7128\n",
      "Epoch 67/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5326 - acc: 0.7258 - val_loss: 0.5516 - val_acc: 0.7128\n",
      "Epoch 68/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5349 - acc: 0.7281 - val_loss: 0.5515 - val_acc: 0.7122\n",
      "Epoch 69/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5397 - acc: 0.7300 - val_loss: 0.5553 - val_acc: 0.7064\n",
      "Epoch 70/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5376 - acc: 0.7240 - val_loss: 0.5606 - val_acc: 0.7091\n",
      "Epoch 71/200\n",
      "10689/10689 [==============================] - 11s 1000us/step - loss: 0.5392 - acc: 0.7285 - val_loss: 0.5548 - val_acc: 0.7091\n",
      "Epoch 72/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5387 - acc: 0.7228 - val_loss: 0.5620 - val_acc: 0.7064\n",
      "Epoch 73/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5370 - acc: 0.7290 - val_loss: 0.5595 - val_acc: 0.7059\n",
      "Epoch 74/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5350 - acc: 0.7300 - val_loss: 0.5546 - val_acc: 0.7112\n",
      "Epoch 75/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5428 - acc: 0.7250 - val_loss: 0.5564 - val_acc: 0.7154\n",
      "Epoch 76/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5345 - acc: 0.7337 - val_loss: 0.5598 - val_acc: 0.7085\n",
      "Epoch 77/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5414 - acc: 0.7261 - val_loss: 0.5617 - val_acc: 0.7128\n",
      "Epoch 78/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5370 - acc: 0.7258 - val_loss: 0.5570 - val_acc: 0.7011\n",
      "Epoch 79/200\n",
      "10689/10689 [==============================] - 11s 1ms/step - loss: 0.5350 - acc: 0.7287 - val_loss: 0.5572 - val_acc: 0.7181\n",
      "Epoch 80/200\n",
      " 2720/10689 [======>.......................] - ETA: 7s - loss: 0.5280 - acc: 0.7316"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e9c48db584c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m conv_model.fit(X_train_cnn, Y_train_cnn, \n\u001b[1;32m      2\u001b[0m                \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                epochs = 200, batch_size = 32, verbose = True)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1190\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                             \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1193\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_slice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "conv_model.fit(X_train_cnn, Y_train_cnn, \n",
    "               validation_split = 0.15,\n",
    "               epochs = 200, batch_size = 32, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm = X_train.reshape((len(X_train), 16, 10, -1), order = 'F')\n",
    "X_train_lstm = X_train_lstm.mean(axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 150)               177000    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 152       \n",
      "=================================================================\n",
      "Total params: 188,477\n",
      "Trainable params: 188,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(150, recurrent_dropout = 0.2, input_shape = (16,144)))\n",
    "lstm_model.add(Dropout(0.75))\n",
    "lstm_model.add(Dense(units = 75, activation = 'relu'))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(units = 2, \n",
    "                activation='softmax'))\n",
    "lstm_model.compile(loss = losses.categorical_crossentropy,\n",
    "                     optimizer = 'adam',\n",
    "                     metrics = ['accuracy'])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10689 samples, validate on 1887 samples\n",
      "Epoch 1/20\n",
      "10689/10689 [==============================] - 5s 422us/step - loss: 0.6781 - acc: 0.5964 - val_loss: 0.6478 - val_acc: 0.6444\n",
      "Epoch 2/20\n",
      "10689/10689 [==============================] - 3s 311us/step - loss: 0.6457 - acc: 0.6283 - val_loss: 0.6337 - val_acc: 0.6603\n",
      "Epoch 3/20\n",
      "10689/10689 [==============================] - 3s 316us/step - loss: 0.6361 - acc: 0.6416 - val_loss: 0.6255 - val_acc: 0.6566\n",
      "Epoch 4/20\n",
      "10689/10689 [==============================] - 3s 308us/step - loss: 0.6240 - acc: 0.6553 - val_loss: 0.6182 - val_acc: 0.6831\n",
      "Epoch 5/20\n",
      "10689/10689 [==============================] - 3s 310us/step - loss: 0.6128 - acc: 0.6700 - val_loss: 0.6100 - val_acc: 0.6751\n",
      "Epoch 6/20\n",
      "10689/10689 [==============================] - 3s 315us/step - loss: 0.6042 - acc: 0.6875 - val_loss: 0.6038 - val_acc: 0.6789\n",
      "Epoch 7/20\n",
      "10689/10689 [==============================] - 3s 314us/step - loss: 0.5956 - acc: 0.6907 - val_loss: 0.5909 - val_acc: 0.6974\n",
      "Epoch 8/20\n",
      "10689/10689 [==============================] - 3s 313us/step - loss: 0.5844 - acc: 0.6997 - val_loss: 0.5921 - val_acc: 0.7043\n",
      "Epoch 9/20\n",
      "10689/10689 [==============================] - 3s 315us/step - loss: 0.5822 - acc: 0.6984 - val_loss: 0.5899 - val_acc: 0.6889\n",
      "Epoch 10/20\n",
      "10689/10689 [==============================] - 3s 318us/step - loss: 0.5776 - acc: 0.7102 - val_loss: 0.5865 - val_acc: 0.7006\n",
      "Epoch 11/20\n",
      "10689/10689 [==============================] - 3s 314us/step - loss: 0.5759 - acc: 0.7124 - val_loss: 0.5857 - val_acc: 0.6995\n",
      "Epoch 12/20\n",
      "10689/10689 [==============================] - 3s 315us/step - loss: 0.5709 - acc: 0.7125 - val_loss: 0.5831 - val_acc: 0.7001\n",
      "Epoch 13/20\n",
      "10689/10689 [==============================] - 3s 320us/step - loss: 0.5676 - acc: 0.7162 - val_loss: 0.5805 - val_acc: 0.7043\n",
      "Epoch 14/20\n",
      "10689/10689 [==============================] - 3s 317us/step - loss: 0.5648 - acc: 0.7188 - val_loss: 0.5851 - val_acc: 0.6942\n",
      "Epoch 15/20\n",
      "10689/10689 [==============================] - 3s 319us/step - loss: 0.5567 - acc: 0.7205 - val_loss: 0.5842 - val_acc: 0.6985\n",
      "Epoch 16/20\n",
      "10689/10689 [==============================] - 3s 319us/step - loss: 0.5511 - acc: 0.7286 - val_loss: 0.5804 - val_acc: 0.6958\n",
      "Epoch 17/20\n",
      "10689/10689 [==============================] - 3s 316us/step - loss: 0.5490 - acc: 0.7285 - val_loss: 0.5776 - val_acc: 0.7006\n",
      "Epoch 18/20\n",
      "10689/10689 [==============================] - 3s 323us/step - loss: 0.5527 - acc: 0.7301 - val_loss: 0.5814 - val_acc: 0.6958\n",
      "Epoch 19/20\n",
      "10689/10689 [==============================] - 3s 320us/step - loss: 0.5410 - acc: 0.7337 - val_loss: 0.5846 - val_acc: 0.6916\n",
      "Epoch 20/20\n",
      "10689/10689 [==============================] - 3s 320us/step - loss: 0.5331 - acc: 0.7378 - val_loss: 0.5784 - val_acc: 0.6937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fba55bd1400>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(X_train_lstm, Y_train_cnn, \n",
    "               validation_split = 0.15,\n",
    "               epochs = 20, batch_size = 64, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12576, 16, 144)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fb9ad1936d8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb9ad3b8c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extract_cnn_features = K.function([lstm_model.layers[0].input, K.learning_phase()],\n",
    "                                  [lstm_model.layers[2].output])\n",
    "X_train_features = extract_cnn_features([X_train_lstm, 0])[0]\n",
    "X_train_features[0,:]\n",
    "\n",
    "proj = PCA(n_components = 2)\n",
    "X_proj = proj.fit_transform(X_train_features)\n",
    "\n",
    "plt.scatter(X_proj[:,0], X_proj[:,1], c= Y_train.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier\n",
    "\n",
    "\n",
    "parameters = {'n_estimators': 200, 'max_depth': None, 'max_features': 15, \n",
    "               'min_samples_split': 15, 'min_samples_leaf': 2, 'bootstrap': True, \n",
    "               'oob_score': True, 'criterion': 'entropy'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77225755, 0.77742448, 0.77256461, 0.77415507, 0.71320605])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "cross_val_score(clf, X_train_features, Y_train.flatten(), cv= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_print(skf_splitter, X, Y, create_model_func, epochs, batch_size, verbose, last_layer):\n",
    "    accs_train_svm = []\n",
    "    accs_val_svm = []\n",
    "    \n",
    "    accs_train_lgr = []\n",
    "    accs_val_lgr = []\n",
    "    \n",
    "    accs_train_xgb = []\n",
    "    accs_val_xgb = []\n",
    "    \n",
    "    for train, val in skf.split(X, Y):\n",
    "        model = create_model_func()\n",
    "        Y_train = np_utils.to_categorical(Y[train], 2)\n",
    "        Y_val = np_utils.to_categorical(Y[val], 2)\n",
    "        model.fit(X[train], Y_train, epochs = epochs, batch_size = batch_size, verbose = verbose)\n",
    "\n",
    "        \n",
    "        extract_cnn_features = K.function([model.layers[0].input, K.learning_phase()],\n",
    "                                  [model.layers[last_layer].output])\n",
    "        X_features = extract_cnn_features([X, 0])[0]\n",
    "        \n",
    "        \n",
    "        clf = SVC(kernel = 'rbf')\n",
    "        clf.fit(X_features[train], Y[train].flatten())\n",
    "        acc_train = clf.score(X_features[train], Y[train].flatten())\n",
    "        accs_train_svm.append(acc_train)\n",
    "        acc_val = clf.score(X_features[val], Y[val].flatten())\n",
    "        accs_val_svm.append(acc_val)\n",
    "        print('Training SVM: {0:.2f} - {1:.2f}'.format(100*acc_train, 100*acc_val))\n",
    "        \n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(X_features[train], Y[train].flatten())\n",
    "        acc_train = clf.score(X_features[train], Y[train].flatten())\n",
    "        accs_train_lgr.append(acc_train)\n",
    "        acc_val = clf.score(X_features[val], Y[val].flatten())\n",
    "        accs_val_lgr.append(acc_val)\n",
    "        print('Training LGR: {0:.2f} - {1:.2f}'.format(100*acc_train, 100*acc_val))\n",
    "        \n",
    "#         clf = XGBClassifier(n_estimators = 100, \n",
    "#                             max_depth = 3, \n",
    "#                             min_child_weight = 5,\n",
    "#                             subsample = 0.7, \n",
    "#                             colsample_bytree = 0.7, \n",
    "#                             silent = True)\n",
    "        clf = RandomForestClassifier(**parameters)\n",
    "        clf.fit(X_features[train], Y[train].flatten())\n",
    "        acc_train = clf.score(X_features[train], Y[train].flatten())\n",
    "        accs_train_xgb.append(acc_train)\n",
    "        acc_val = clf.score(X_features[val], Y[val].flatten())\n",
    "        accs_val_xgb.append(acc_val)\n",
    "        print('Training XGBoost: {0:.2f} - {1:.2f}'.format(100*acc_train, 100*acc_val))\n",
    "\n",
    "        \n",
    "    print('SVM')\n",
    "    print('--- Mean cv train score: {0:.2f}% +/- {1:.2f}%'.format(100*np.mean(accs_train_svm), \n",
    "                                                                  100*np.std(accs_train_svm)))\n",
    "    print('--- Mean cv val score: {0:.2f}% +/- {1:.2f}%'.format(100*np.mean(accs_val_svm), \n",
    "                                                                  100*np.std(accs_val_svm)))\n",
    "    \n",
    "    print('LGR')\n",
    "    print('--- Mean cv train score: {0:.2f}% +/- {1:.2f}%'.format(100*np.mean(accs_train_lgr), \n",
    "                                                                  100*np.std(accs_train_lgr)))\n",
    "    print('--- Mean cv val score: {0:.2f}% +/- {1:.2f}%'.format(100*np.mean(accs_val_lgr), \n",
    "                                                                  100*np.std(accs_val_lgr)))\n",
    "    \n",
    "    print('XGBoost')\n",
    "    print('--- Mean cv train score: {0:.2f}% +/- {1:.2f}%'.format(100*np.mean(accs_train_xgb), \n",
    "                                                                  100*np.std(accs_train_xgb)))\n",
    "    print('--- Mean cv val score: {0:.2f}% +/- {1:.2f}%'.format(100*np.mean(accs_val_xgb), \n",
    "                                                                  100*np.std(accs_val_xgb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "10060/10060 [==============================] - 8s 760us/step - loss: 0.6515 - acc: 0.6565\n",
      "Epoch 2/30\n",
      "10060/10060 [==============================] - 5s 474us/step - loss: 0.6038 - acc: 0.6922\n",
      "Epoch 3/30\n",
      "10060/10060 [==============================] - 5s 477us/step - loss: 0.5925 - acc: 0.6995\n",
      "Epoch 4/30\n",
      "10060/10060 [==============================] - 5s 477us/step - loss: 0.5893 - acc: 0.7035\n",
      "Epoch 5/30\n",
      "10060/10060 [==============================] - 5s 477us/step - loss: 0.5841 - acc: 0.7070\n",
      "Epoch 6/30\n",
      "10060/10060 [==============================] - 5s 477us/step - loss: 0.5730 - acc: 0.7132\n",
      "Epoch 7/30\n",
      "10060/10060 [==============================] - 5s 478us/step - loss: 0.5774 - acc: 0.7094\n",
      "Epoch 8/30\n",
      "10060/10060 [==============================] - 5s 475us/step - loss: 0.5809 - acc: 0.7091\n",
      "Epoch 9/30\n",
      "10060/10060 [==============================] - 5s 476us/step - loss: 0.5757 - acc: 0.7118\n",
      "Epoch 10/30\n",
      "10060/10060 [==============================] - 5s 478us/step - loss: 0.5757 - acc: 0.7139\n",
      "Epoch 11/30\n",
      "10060/10060 [==============================] - 5s 478us/step - loss: 0.5730 - acc: 0.7117\n",
      "Epoch 12/30\n",
      "10060/10060 [==============================] - 5s 478us/step - loss: 0.5681 - acc: 0.7122\n",
      "Epoch 13/30\n",
      "10060/10060 [==============================] - 5s 479us/step - loss: 0.5697 - acc: 0.7145\n",
      "Epoch 14/30\n",
      "10060/10060 [==============================] - 5s 479us/step - loss: 0.5660 - acc: 0.7133\n",
      "Epoch 15/30\n",
      "10060/10060 [==============================] - 5s 478us/step - loss: 0.5667 - acc: 0.7153\n",
      "Epoch 16/30\n",
      "10060/10060 [==============================] - 5s 479us/step - loss: 0.5658 - acc: 0.7174\n",
      "Epoch 17/30\n",
      "10060/10060 [==============================] - 5s 479us/step - loss: 0.5689 - acc: 0.7195\n",
      "Epoch 18/30\n",
      "10060/10060 [==============================] - 5s 478us/step - loss: 0.5675 - acc: 0.7188\n",
      "Epoch 19/30\n",
      "10060/10060 [==============================] - 5s 488us/step - loss: 0.5681 - acc: 0.7173\n",
      "Epoch 20/30\n",
      "10060/10060 [==============================] - 5s 476us/step - loss: 0.5628 - acc: 0.7201\n",
      "Epoch 21/30\n",
      "10060/10060 [==============================] - 5s 476us/step - loss: 0.5630 - acc: 0.7197\n",
      "Epoch 22/30\n",
      "10060/10060 [==============================] - 5s 476us/step - loss: 0.5578 - acc: 0.7207\n",
      "Epoch 23/30\n",
      "10060/10060 [==============================] - 5s 479us/step - loss: 0.5559 - acc: 0.7207\n",
      "Epoch 24/30\n",
      "10060/10060 [==============================] - 5s 481us/step - loss: 0.5602 - acc: 0.7216\n",
      "Epoch 25/30\n",
      "10060/10060 [==============================] - 5s 500us/step - loss: 0.5516 - acc: 0.7263\n",
      "Epoch 26/30\n",
      "10060/10060 [==============================] - 5s 488us/step - loss: 0.5538 - acc: 0.7227\n",
      "Epoch 27/30\n",
      "10060/10060 [==============================] - 5s 485us/step - loss: 0.5565 - acc: 0.7258\n",
      "Epoch 28/30\n",
      "10060/10060 [==============================] - 5s 484us/step - loss: 0.5523 - acc: 0.7251\n",
      "Epoch 29/30\n",
      "10060/10060 [==============================] - 5s 482us/step - loss: 0.5522 - acc: 0.7249\n",
      "Epoch 30/30\n",
      "10060/10060 [==============================] - 5s 482us/step - loss: 0.5467 - acc: 0.7288\n",
      "Training SVM: 74.08 - 73.65\n",
      "Training LGR: 75.35 - 73.37\n",
      "Training XGBoost: 95.26 - 74.21\n",
      "Epoch 1/30\n",
      "10060/10060 [==============================] - 8s 782us/step - loss: 0.6627 - acc: 0.6527\n",
      "Epoch 2/30\n",
      "10060/10060 [==============================] - 5s 495us/step - loss: 0.6030 - acc: 0.6911\n",
      "Epoch 3/30\n",
      "10060/10060 [==============================] - 5s 484us/step - loss: 0.5884 - acc: 0.7021\n",
      "Epoch 4/30\n",
      "10060/10060 [==============================] - 5s 465us/step - loss: 0.5848 - acc: 0.7092\n",
      "Epoch 5/30\n",
      "10060/10060 [==============================] - 5s 462us/step - loss: 0.5743 - acc: 0.7120\n",
      "Epoch 6/30\n",
      "10060/10060 [==============================] - 5s 506us/step - loss: 0.5764 - acc: 0.7130\n",
      "Epoch 7/30\n",
      "10060/10060 [==============================] - 5s 477us/step - loss: 0.5758 - acc: 0.7123\n",
      "Epoch 8/30\n",
      "10060/10060 [==============================] - 5s 462us/step - loss: 0.5729 - acc: 0.7147\n",
      "Epoch 9/30\n",
      "10060/10060 [==============================] - 5s 461us/step - loss: 0.5704 - acc: 0.7117\n",
      "Epoch 10/30\n",
      "10060/10060 [==============================] - 5s 461us/step - loss: 0.5727 - acc: 0.7182\n",
      "Epoch 11/30\n",
      "10060/10060 [==============================] - 5s 462us/step - loss: 0.5689 - acc: 0.7173\n",
      "Epoch 12/30\n",
      "10060/10060 [==============================] - 5s 462us/step - loss: 0.5638 - acc: 0.7188\n",
      "Epoch 13/30\n",
      "10060/10060 [==============================] - 5s 462us/step - loss: 0.5657 - acc: 0.7203\n",
      "Epoch 14/30\n",
      "10060/10060 [==============================] - 5s 461us/step - loss: 0.5681 - acc: 0.7185\n",
      "Epoch 15/30\n",
      "10060/10060 [==============================] - 5s 463us/step - loss: 0.5632 - acc: 0.7188\n",
      "Epoch 16/30\n",
      "10060/10060 [==============================] - 5s 462us/step - loss: 0.5612 - acc: 0.7204\n",
      "Epoch 17/30\n",
      "10060/10060 [==============================] - 5s 462us/step - loss: 0.5603 - acc: 0.7210\n",
      "Epoch 18/30\n",
      "10060/10060 [==============================] - 5s 462us/step - loss: 0.5600 - acc: 0.7185\n",
      "Epoch 19/30\n",
      "10060/10060 [==============================] - 5s 461us/step - loss: 0.5599 - acc: 0.7211\n",
      "Epoch 20/30\n",
      "10060/10060 [==============================] - 5s 472us/step - loss: 0.5601 - acc: 0.7224\n",
      "Epoch 21/30\n",
      "10060/10060 [==============================] - 5s 505us/step - loss: 0.5586 - acc: 0.7168\n",
      "Epoch 22/30\n",
      "10060/10060 [==============================] - 5s 488us/step - loss: 0.5586 - acc: 0.7242\n",
      "Epoch 23/30\n",
      "10060/10060 [==============================] - 5s 490us/step - loss: 0.5555 - acc: 0.7228\n",
      "Epoch 24/30\n",
      "10060/10060 [==============================] - 5s 490us/step - loss: 0.5546 - acc: 0.7252\n",
      "Epoch 25/30\n",
      "10060/10060 [==============================] - 5s 487us/step - loss: 0.5561 - acc: 0.7224\n",
      "Epoch 26/30\n",
      "10060/10060 [==============================] - 5s 489us/step - loss: 0.5543 - acc: 0.7237\n",
      "Epoch 27/30\n",
      "10060/10060 [==============================] - 5s 489us/step - loss: 0.5517 - acc: 0.7243\n",
      "Epoch 28/30\n",
      "10060/10060 [==============================] - 5s 488us/step - loss: 0.5484 - acc: 0.7269\n",
      "Epoch 29/30\n",
      "10060/10060 [==============================] - 5s 487us/step - loss: 0.5472 - acc: 0.7299\n",
      "Epoch 30/30\n",
      "10060/10060 [==============================] - 5s 487us/step - loss: 0.5448 - acc: 0.7325\n",
      "Training SVM: 74.94 - 73.09\n",
      "Training LGR: 75.73 - 72.50\n",
      "Training XGBoost: 95.64 - 73.73\n",
      "Epoch 1/30\n",
      "10061/10061 [==============================] - 8s 808us/step - loss: 0.6604 - acc: 0.6565\n",
      "Epoch 2/30\n",
      "10061/10061 [==============================] - 5s 478us/step - loss: 0.5973 - acc: 0.6953\n",
      "Epoch 3/30\n",
      "10061/10061 [==============================] - 5s 510us/step - loss: 0.5903 - acc: 0.7008\n",
      "Epoch 4/30\n",
      "10061/10061 [==============================] - 5s 500us/step - loss: 0.5776 - acc: 0.7107\n",
      "Epoch 5/30\n",
      "10061/10061 [==============================] - 5s 490us/step - loss: 0.5796 - acc: 0.7130\n",
      "Epoch 6/30\n",
      "10061/10061 [==============================] - 5s 487us/step - loss: 0.5757 - acc: 0.7141\n",
      "Epoch 7/30\n",
      "10061/10061 [==============================] - 5s 491us/step - loss: 0.5727 - acc: 0.7147\n",
      "Epoch 8/30\n",
      "10061/10061 [==============================] - 5s 491us/step - loss: 0.5718 - acc: 0.7185\n",
      "Epoch 9/30\n",
      "10061/10061 [==============================] - 5s 492us/step - loss: 0.5711 - acc: 0.7153\n",
      "Epoch 10/30\n",
      "10061/10061 [==============================] - 5s 485us/step - loss: 0.5678 - acc: 0.7207\n",
      "Epoch 11/30\n",
      "10061/10061 [==============================] - 5s 488us/step - loss: 0.5608 - acc: 0.7187\n",
      "Epoch 12/30\n",
      "10061/10061 [==============================] - 5s 487us/step - loss: 0.5638 - acc: 0.7212\n",
      "Epoch 13/30\n",
      "10061/10061 [==============================] - 5s 486us/step - loss: 0.5610 - acc: 0.7195\n",
      "Epoch 14/30\n",
      "10061/10061 [==============================] - 5s 488us/step - loss: 0.5614 - acc: 0.7212\n",
      "Epoch 15/30\n",
      "10061/10061 [==============================] - 5s 489us/step - loss: 0.5616 - acc: 0.7224\n",
      "Epoch 16/30\n",
      "10061/10061 [==============================] - 5s 486us/step - loss: 0.5643 - acc: 0.7200\n",
      "Epoch 17/30\n",
      "10061/10061 [==============================] - 5s 485us/step - loss: 0.5590 - acc: 0.7199\n",
      "Epoch 18/30\n",
      "10061/10061 [==============================] - 5s 489us/step - loss: 0.5567 - acc: 0.7264\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10061/10061 [==============================] - 5s 475us/step - loss: 0.5599 - acc: 0.7252\n",
      "Epoch 20/30\n",
      "10061/10061 [==============================] - 5s 473us/step - loss: 0.5561 - acc: 0.7230\n",
      "Epoch 21/30\n",
      "10061/10061 [==============================] - 5s 472us/step - loss: 0.5609 - acc: 0.7228\n",
      "Epoch 22/30\n",
      "10061/10061 [==============================] - 5s 469us/step - loss: 0.5535 - acc: 0.7267\n",
      "Epoch 23/30\n",
      "10061/10061 [==============================] - 5s 469us/step - loss: 0.5510 - acc: 0.7253\n",
      "Epoch 24/30\n",
      "10061/10061 [==============================] - 5s 470us/step - loss: 0.5518 - acc: 0.7267\n",
      "Epoch 25/30\n",
      "10061/10061 [==============================] - 5s 473us/step - loss: 0.5545 - acc: 0.7247\n",
      "Epoch 26/30\n",
      "10061/10061 [==============================] - 5s 471us/step - loss: 0.5480 - acc: 0.7257\n",
      "Epoch 27/30\n",
      "10061/10061 [==============================] - 5s 472us/step - loss: 0.5479 - acc: 0.7294\n",
      "Epoch 28/30\n",
      "10061/10061 [==============================] - 5s 472us/step - loss: 0.5440 - acc: 0.7297\n",
      "Epoch 29/30\n",
      "10061/10061 [==============================] - 5s 470us/step - loss: 0.5440 - acc: 0.7269\n",
      "Epoch 30/30\n",
      "10061/10061 [==============================] - 5s 471us/step - loss: 0.5445 - acc: 0.7343\n",
      "Training SVM: 74.78 - 71.73\n",
      "Training LGR: 75.64 - 71.61\n",
      "Training XGBoost: 95.34 - 72.29\n",
      "Epoch 1/30\n",
      "10061/10061 [==============================] - 8s 799us/step - loss: 0.6430 - acc: 0.6647\n",
      "Epoch 2/30\n",
      "10061/10061 [==============================] - 5s 473us/step - loss: 0.5998 - acc: 0.6947\n",
      "Epoch 3/30\n",
      "10061/10061 [==============================] - 5s 474us/step - loss: 0.5894 - acc: 0.6975\n",
      "Epoch 4/30\n",
      "10061/10061 [==============================] - 5s 472us/step - loss: 0.5807 - acc: 0.7078\n",
      "Epoch 5/30\n",
      "10061/10061 [==============================] - 5s 470us/step - loss: 0.5803 - acc: 0.7084\n",
      "Epoch 6/30\n",
      "10061/10061 [==============================] - 5s 473us/step - loss: 0.5722 - acc: 0.7139\n",
      "Epoch 7/30\n",
      "10061/10061 [==============================] - 5s 475us/step - loss: 0.5751 - acc: 0.7146\n",
      "Epoch 8/30\n",
      "10061/10061 [==============================] - 5s 474us/step - loss: 0.5661 - acc: 0.7193\n",
      "Epoch 9/30\n",
      "10061/10061 [==============================] - 5s 474us/step - loss: 0.5685 - acc: 0.7195\n",
      "Epoch 10/30\n",
      "10061/10061 [==============================] - 5s 471us/step - loss: 0.5651 - acc: 0.7173\n",
      "Epoch 11/30\n",
      "10061/10061 [==============================] - 5s 476us/step - loss: 0.5647 - acc: 0.7227\n",
      "Epoch 12/30\n",
      "10061/10061 [==============================] - 5s 477us/step - loss: 0.5664 - acc: 0.7185\n",
      "Epoch 13/30\n",
      "10061/10061 [==============================] - 5s 508us/step - loss: 0.5651 - acc: 0.7160\n",
      "Epoch 14/30\n",
      "10061/10061 [==============================] - 5s 504us/step - loss: 0.5640 - acc: 0.7224\n",
      "Epoch 15/30\n",
      "10061/10061 [==============================] - 5s 491us/step - loss: 0.5607 - acc: 0.7216\n",
      "Epoch 16/30\n",
      "10061/10061 [==============================] - 5s 466us/step - loss: 0.5607 - acc: 0.7207\n",
      "Epoch 17/30\n",
      "10061/10061 [==============================] - 5s 499us/step - loss: 0.5587 - acc: 0.7196\n",
      "Epoch 18/30\n",
      "10061/10061 [==============================] - 5s 498us/step - loss: 0.5557 - acc: 0.7224\n",
      "Epoch 19/30\n",
      "10061/10061 [==============================] - 5s 500us/step - loss: 0.5541 - acc: 0.7199\n",
      "Epoch 20/30\n",
      "10061/10061 [==============================] - 5s 497us/step - loss: 0.5596 - acc: 0.7218\n",
      "Epoch 21/30\n",
      "10061/10061 [==============================] - 5s 500us/step - loss: 0.5555 - acc: 0.7246\n",
      "Epoch 22/30\n",
      "10061/10061 [==============================] - 5s 486us/step - loss: 0.5516 - acc: 0.7288\n",
      "Epoch 23/30\n",
      "10061/10061 [==============================] - 5s 489us/step - loss: 0.5483 - acc: 0.7272\n",
      "Epoch 24/30\n",
      "10061/10061 [==============================] - 5s 485us/step - loss: 0.5533 - acc: 0.7272\n",
      "Epoch 25/30\n",
      "10061/10061 [==============================] - 5s 489us/step - loss: 0.5436 - acc: 0.7296\n",
      "Epoch 26/30\n",
      "10061/10061 [==============================] - 5s 487us/step - loss: 0.5432 - acc: 0.7343\n",
      "Epoch 27/30\n",
      "10061/10061 [==============================] - 5s 496us/step - loss: 0.5465 - acc: 0.7312\n",
      "Epoch 28/30\n",
      "10061/10061 [==============================] - 5s 507us/step - loss: 0.5427 - acc: 0.7320\n",
      "Epoch 29/30\n",
      "10061/10061 [==============================] - 5s 517us/step - loss: 0.5399 - acc: 0.7356\n",
      "Epoch 30/30\n",
      "10061/10061 [==============================] - 5s 508us/step - loss: 0.5348 - acc: 0.7308\n",
      "Training SVM: 75.81 - 71.61\n",
      "Training LGR: 76.68 - 71.61\n",
      "Training XGBoost: 95.47 - 73.20\n",
      "Epoch 1/30\n",
      "10062/10062 [==============================] - 8s 844us/step - loss: 0.6468 - acc: 0.6663\n",
      "Epoch 2/30\n",
      "10062/10062 [==============================] - 5s 474us/step - loss: 0.5984 - acc: 0.6931\n",
      "Epoch 3/30\n",
      "10062/10062 [==============================] - 5s 465us/step - loss: 0.5834 - acc: 0.7089\n",
      "Epoch 4/30\n",
      "10062/10062 [==============================] - 5s 464us/step - loss: 0.5755 - acc: 0.7177\n",
      "Epoch 5/30\n",
      "10062/10062 [==============================] - 5s 464us/step - loss: 0.5772 - acc: 0.7141\n",
      "Epoch 6/30\n",
      "10062/10062 [==============================] - 5s 463us/step - loss: 0.5710 - acc: 0.7217\n",
      "Epoch 7/30\n",
      "10062/10062 [==============================] - 5s 508us/step - loss: 0.5622 - acc: 0.7259\n",
      "Epoch 8/30\n",
      "10062/10062 [==============================] - 5s 463us/step - loss: 0.5641 - acc: 0.7237\n",
      "Epoch 9/30\n",
      "10062/10062 [==============================] - 5s 466us/step - loss: 0.5653 - acc: 0.7229\n",
      "Epoch 10/30\n",
      "10062/10062 [==============================] - 5s 481us/step - loss: 0.5633 - acc: 0.7249\n",
      "Epoch 11/30\n",
      "10062/10062 [==============================] - 5s 480us/step - loss: 0.5602 - acc: 0.7229\n",
      "Epoch 12/30\n",
      "10062/10062 [==============================] - 5s 483us/step - loss: 0.5570 - acc: 0.7298\n",
      "Epoch 13/30\n",
      "10062/10062 [==============================] - 5s 495us/step - loss: 0.5614 - acc: 0.7269\n",
      "Epoch 14/30\n",
      "10062/10062 [==============================] - 5s 514us/step - loss: 0.5589 - acc: 0.7300\n",
      "Epoch 15/30\n",
      "10062/10062 [==============================] - 5s 471us/step - loss: 0.5567 - acc: 0.7292\n",
      "Epoch 16/30\n",
      "10062/10062 [==============================] - 5s 519us/step - loss: 0.5560 - acc: 0.7317\n",
      "Epoch 17/30\n",
      "10062/10062 [==============================] - 5s 517us/step - loss: 0.5522 - acc: 0.7302\n",
      "Epoch 18/30\n",
      "10062/10062 [==============================] - 5s 474us/step - loss: 0.5563 - acc: 0.7270\n",
      "Epoch 19/30\n",
      "10062/10062 [==============================] - 5s 506us/step - loss: 0.5538 - acc: 0.7301\n",
      "Epoch 20/30\n",
      "10062/10062 [==============================] - 5s 490us/step - loss: 0.5535 - acc: 0.7309\n",
      "Epoch 21/30\n",
      "10062/10062 [==============================] - 5s 490us/step - loss: 0.5501 - acc: 0.7307\n",
      "Epoch 22/30\n",
      "10062/10062 [==============================] - 5s 493us/step - loss: 0.5508 - acc: 0.7329\n",
      "Epoch 23/30\n",
      "10062/10062 [==============================] - 5s 490us/step - loss: 0.5515 - acc: 0.7320\n",
      "Epoch 24/30\n",
      "10062/10062 [==============================] - 5s 489us/step - loss: 0.5450 - acc: 0.7387\n",
      "Epoch 25/30\n",
      "10062/10062 [==============================] - 5s 489us/step - loss: 0.5469 - acc: 0.7365\n",
      "Epoch 26/30\n",
      "10062/10062 [==============================] - 5s 488us/step - loss: 0.5455 - acc: 0.7372\n",
      "Epoch 27/30\n",
      "10062/10062 [==============================] - 5s 487us/step - loss: 0.5407 - acc: 0.7429\n",
      "Epoch 28/30\n",
      "10062/10062 [==============================] - 5s 485us/step - loss: 0.5442 - acc: 0.7378\n",
      "Epoch 29/30\n",
      "10062/10062 [==============================] - 5s 491us/step - loss: 0.5332 - acc: 0.7389\n",
      "Epoch 30/30\n",
      "10062/10062 [==============================] - 5s 495us/step - loss: 0.5382 - acc: 0.7393\n",
      "Training SVM: 75.95 - 70.09\n",
      "Training LGR: 76.86 - 70.56\n",
      "Training XGBoost: 95.15 - 72.12\n",
      "SVM\n",
      "--- Mean cv train score: 75.11% +/- 0.69%\n",
      "--- Mean cv val score: 72.03% +/- 1.25%\n",
      "LGR\n",
      "--- Mean cv train score: 76.05% +/- 0.60%\n",
      "--- Mean cv val score: 71.93% +/- 0.94%\n",
      "XGBoost\n",
      "--- Mean cv train score: 95.37% +/- 0.17%\n",
      "--- Mean cv val score: 73.11% +/- 0.81%\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(Bidirectional(LSTM(150, recurrent_dropout = 0.25), input_shape = (16,144)))\n",
    "    lstm_model.add(Dropout(0.75))\n",
    "    lstm_model.add(Dense(units = 100, activation = 'relu'))\n",
    "    lstm_model.add(Dropout(0.5))\n",
    "    lstm_model.add(Dense(units = 2, \n",
    "                    activation='softmax'))\n",
    "    lstm_model.compile(loss = losses.categorical_crossentropy,\n",
    "                         optimizer = 'adam',\n",
    "                         metrics = ['accuracy'])\n",
    "    return lstm_model\n",
    "\n",
    "kfold_print(skf, X_train_lstm, Y_train, create_model, 30, 64, True, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
